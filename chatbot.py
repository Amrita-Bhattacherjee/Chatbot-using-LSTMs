# -*- coding: utf-8 -*-
"""ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cg4li53WO2UdWXDk1_EYfUT91A7bYCNv

#Creating a Chatbot Model

Using LSTM, Neural networks and Softmax Classifier as activation function
"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential,Model
from keras.layers import Embedding
from keras.layers import Input,Activation,Dense,Permute,Dropout,add,dot,concatenate,LSTM

#Loading the training set
with open("Train.txt","rb") as fp:
  train_data = pickle.load(fp)

print(train_data)

#Loading the test set
with open("Test.txt","rb") as fp:
  test_data = pickle.load(fp)

print(test_data)

#Exploring the data's attributes
print("Training dataset is of type", type(train_data))
print("Test dataset is of type", type(test_data))

print("Length of training data = ",len(train_data))
print("Length of test data = ",len(test_data))

#Setting up vocabulary
vocab = set()

all_data = test_data + train_data
for story,question,answer in all_data:
  vocab = vocab.union(set(story))
  vocab = vocab.union(set(question))

vocab.add('yes')
vocab.add('no')

vocab_len = len(vocab)+1

max_story_length = max([len(data[0]) for data in all_data])
print("Maximum story length = ",max_story_length)

max_qs_length = max([len(data[1]) for data in all_data])
print("Maximum question length = ",max_qs_length)

tokenizer = Tokenizer(filters=[])
tokenizer.fit_on_texts(vocab)
tokenizer.word_index

train_story_text = []
train_qs_text = []
train_ans = []

for story,question,answer in train_data:
  train_story_text.append(story)
  train_qs_text.append(question)

train_story_seq = tokenizer.texts_to_sequences(train_story_text)

def vectorize_stories(data,word_index=tokenizer.word_index,
                      max_story_length=max_story_length,
                      max_qs_length = max_qs_length):

  X = []          #Stories
  Xq = []         #Question
  Y = []          #Correct answer

  for story,query,answer in data:
    x = [word_index[word.lower()] for word in story]
    xq = [word_index[word.lower()] for word in query]
    y = np.zeros(len(word_index)+1)
    y[word_index[answer]] = 1

    X.append(x)
    Xq.append(xq)
    Y.append(y)

  return (pad_sequences(X, maxlen = max_story_length),pad_sequences(Xq,maxlen = max_qs_length),np.array(Y))

#Vectorizing the data

inputs_train, queries_train, answers_train = vectorize_stories(train_data)
inputs_test, queries_test, answers_test = vectorize_stories(test_data)

#Model

input_sequence = Input((max_story_length,))
question = Input((max_qs_length,))

#Input encoder m
input_encoder_m = Sequential()
input_encoder_m.add(Embedding(input_dim=vocab_len,output_dim=64))
input_encoder_m.add(Dropout(0.3))

#Input encoder c
input_encoder_c = Sequential()
input_encoder_c.add(Embedding(input_dim=vocab_len,output_dim=max_qs_length))
input_encoder_c.add(Dropout(0.3))

#Question encoder
question_encoder = Sequential()
question_encoder.add(Embedding(input_dim=vocab_len,output_dim=64,input_length=max_qs_length))
question_encoder.add(Dropout(0.3))

#Encode the sequences
input_encoded_m = input_encoder_m(input_sequence)
input_encoded_c = input_encoder_c(input_sequence)
question_encoded = question_encoder(question)

match = dot([input_encoded_m,question_encoded],axes = (2,2))
match = Activation('softmax')(match)

response = add([match,input_encoded_c])
response = Permute((2,1))(response)

#Concatenate
answer = concatenate([response,question_encoded])
answer = LSTM(32)(answer)
answer = Dropout(0.5)(answer)
answer = Dense(vocab_len)(answer)
answer = Activation('softmax')(answer)
model = Model([input_sequence,question],answer)
model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',metrics = ['accuracy'])
model.summary()

history = model.fit([inputs_train, queries_train], answers_train, batch_size = 32,
                    epochs = 20, validation_data = ([inputs_test,queries_test],answers_test))

print(history.history.keys())
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title("Model Accuracy")
plt.ylabel("Accuracy")
plt.xlabel("epochs")

#Saving the model

model.save("Chatbot_model")

#Evaluation on the Test Set
model.load_weights("Chatbot_model")
pred_results = model.predict(([inputs_test, queries_test]))

#Example
story = ' '.join(word for word in test_data[10][0])
query = ' '.join(word for word in test_data[10][1])
print(story)
print(query)
test_data[10][2]

#Check probability of certainty for a predicted answer
val_max = np.argmax(pred_results[0])

for key,val in tokenizer.word_index.items():
  if val==val_max:
    k=key

print("Predicted Answer is ",k)
print("Probability of certainty is ",pred_results[13][val_max])

#Create new story and predict answer

story = "Mary dropped the football . Sandra discarded apple in the kitchen . Daniel went to office"
my_question = "Is Daniel in the kitchen ?"

mydata = [(story.split(),my_question.split(),'yes')]
my_story,my_ques,my_ans = vectorize_stories(mydata)
pred_results = model.predict(([my_story,my_ques]))

val_max = np.argmax(pred_results[0])

for key,val in tokenizer.word_index.items():
  if val==val_max:
    k=key

print("Predicted Answer is ",k)
print("Probability of certainty is ",pred_results[0][val_max])